{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5Q+Ki0aNnYb9cdjG73N9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Debanil1986/Joint-Multipathpp_testing/blob/master/EMMA_with_Camera_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, output_dim=512):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),  # Reduce spatial size by half\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Further reduce spatial size\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling to output 1x1 feature map\n",
        "        )\n",
        "        self.fc = nn.Linear(256, output_dim)  # Fully connected layer to reduce to output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten the spatial dimensions\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "A58OjlgfwMer"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "\n",
        "def load_pretrained_object_detector():\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    return model\n",
        "\n",
        "object_detector = load_pretrained_object_detector()  # Load the model at the start of your script or main function"
      ],
      "metadata": {
        "id": "ji5w_T1ZJtuF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCeKbK2rmG2m",
        "outputId": "7f5d4fb6-7aaa-4396-d420-a3ddda257831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Want to see lane or objects?lane\n",
            "Number of Frames in Video:  199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199/199 [06:32<00:00,  1.97s/frames]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output video saved to /content/emma_processedvideos.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "cnn_feature_dim = 512\n",
        "intent_dim = 10\n",
        "historical_state_dim = 4\n",
        "hidden_size = 512\n",
        "resized_width, resized_height = 640, 480\n",
        "\n",
        "\n",
        "class EMMA:\n",
        "    def __init__(self, cnn_feature_dim, intent_dim, historical_state_dim,hidden_size):\n",
        "        super(EMMA, self).__init__()\n",
        "        self.cnn_feature_dim = 512  # Desired output feature size from CNN\n",
        "        self.cnn = CustomCNN(output_dim=self.cnn_feature_dim)  # Use the custom CNN\n",
        "        self.rnn_input_size = cnn_feature_dim + intent_dim + historical_state_dim\n",
        "        self.rnn = torch.nn.LSTM(input_size=self.rnn_input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        \"\"\"Resize and normalize the frame.\"\"\"\n",
        "        # Example preprocessing: resize and normalize\n",
        "        resized_frame = cv2.resize(frame, (resized_width, resized_height))\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "        return normalized_frame\n",
        "\n",
        "    def predict(self, frame, intents, historical_states):\n",
        "        \"\"\"Make a prediction using the preprocessed frame.\"\"\"\n",
        "        # Convert frame to a batch format (batch size 1)\n",
        "        camera_frames = frame\n",
        "\n",
        "        camera_frames_tensor = torch.tensor(camera_frames, dtype=torch.float32)\n",
        "\n",
        "        batch_size, T, W, H, C = camera_frames_tensor.shape\n",
        "        cnn_out = self.cnn(camera_frames_tensor.view(-1, C, H, W))  # Reshape and pass through CNN\n",
        "        cnn_out = cnn_out.view(batch_size, T, -1)\n",
        "\n",
        "        # Combine CNN output with intents and historical_states\n",
        "        # Here you might need to encode intents and concatenate\n",
        "        intents_tensor = torch.tensor(intents, dtype=torch.float32)  # Shape: (batch, time, intent_dim)\n",
        "        historical_states_tensor = torch.tensor(historical_states, dtype=torch.float32)  # Shape: (batch, time, state_dim)\n",
        "\n",
        "        combined_features = torch.cat((cnn_out, intents_tensor, historical_states_tensor), dim=-1)\n",
        "\n",
        "        rnn_out, _ = self.rnn(combined_features)\n",
        "        output = self.fc(rnn_out)\n",
        "        return output\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Extract frames from a video and process them with the model.\"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            preprocessed_frame = self.preprocess_frame(frame)\n",
        "            intents = np.random.rand(1, 1, intent_dim)  # Random intents\n",
        "            historical_states = np.random.rand(1, 1, historical_state_dim)\n",
        "            output = self.predict(preprocessed_frame, intents, historical_states)\n",
        "            print(output)  # Print or further process the output\n",
        "            # Display frame\n",
        "            cv2.imshow('Video Frame', frame)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Resize and normalize the frame.\"\"\"\n",
        "    # Resize the frame to the required input size of the model\n",
        "    resized_frame = cv2.resize(frame, (resized_width, resized_height))  # Example resize\n",
        "    # Normalize the frame if necessary\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=0)  # Add batch dimension\n",
        "    preprocessed_frame = np.expand_dims(preprocessed_frame, axis=0)  # Add time dimension\n",
        "    return preprocessed_frame\n",
        "\n",
        "def draw_lane_overlay(frame, lane_points):\n",
        "    \"\"\"Draws a semi-transparent lane overlay.\"\"\"\n",
        "    overlay = frame.copy()\n",
        "    cv2.fillPoly(overlay, [np.array(lane_points, np.int32)], (0, 255, 0))\n",
        "    alpha = 0.4  # Transparency factor.\n",
        "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
        "\n",
        "def adjust_lane_points(frame_width, frame_height):\n",
        "    # Example adjustment, these points should be dynamically calculated based on actual lane detection\n",
        "    return [\n",
        "        (frame_width * 0.4, frame_height),  # Bottom left\n",
        "        (frame_width * 0.6, frame_height),  # Bottom right\n",
        "        (frame_width * 0.55, frame_height * 0.7),  # Top right\n",
        "        (frame_width * 0.45, frame_height * 0.7)   # Top left\n",
        "    ]\n",
        "\n",
        "def preprocess_frame_for_torch(frame):\n",
        "    \"\"\"Preprocess the frame for PyTorch model input.\"\"\"\n",
        "    # Convert frame to RGB (PyTorch models expect RGB)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Resize the frame to the required input size of the model\n",
        "    resized_frame = cv2.resize(frame_rgb, (resized_width, resized_height))  # Example resize\n",
        "    # Normalize the frame to 0-1\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    # Convert to tensor\n",
        "    tensor_frame = torch.from_numpy(normalized_frame).float()\n",
        "    # Rearrange dimensions to (C, H, W) from (H, W, C)\n",
        "    tensor_frame = tensor_frame.permute(2, 0, 1).unsqueeze(0)  # Add batch dimension\n",
        "    return tensor_frame\n",
        "\n",
        "\n",
        "def process_video(video_path, output_video_path, model, intent_dim, historical_state_dim,user_choice=\"both\"):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    lengthOfFrames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(\"Number of Frames in Video: \",lengthOfFrames)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Unable to open the video.\")\n",
        "        return\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'XVID'), fps, (frame_width, frame_height))\n",
        "    pbar = tqdm(total=lengthOfFrames, unit=\"frames\")\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        preprocessed_frame = preprocess_frame(frame)\n",
        "        preprocessed_tensor = preprocess_frame_for_torch(frame)\n",
        "        with torch.no_grad():\n",
        "          detection_output = object_detector(preprocessed_tensor)\n",
        "          detections = detection_output[0]\n",
        "\n",
        "        # Draw detections with high confidence scores\n",
        "        if(user_choice == \"both\" or user_choice==\"box\"):\n",
        "          labels = detections['labels'].cpu().numpy()\n",
        "          boxes = detections['boxes'].cpu().numpy()\n",
        "          scores = detections['scores'].cpu().numpy()\n",
        "          scale_width = frame_width / resized_width\n",
        "          scale_height = frame_height / resized_height\n",
        "          for label, box, score in zip(labels, boxes, scores):\n",
        "              # print(\"SCORE-->\",score)\n",
        "              if score > 0.9:\n",
        "                  x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                  x1 = int(x1 * scale_width)\n",
        "                  y1 = int(y1 * scale_height)\n",
        "                  x2 = int(x2 * scale_width)\n",
        "                  y2 = int(y2 * scale_height)\n",
        "                  cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                  cv2.putText(frame, f\"Car: {score:.2f}\", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        intents = np.random.rand(1, 1, intent_dim)\n",
        "        historical_states = np.random.rand(1, 1, historical_state_dim)\n",
        "        output = model.predict(preprocessed_frame, intents, historical_states)\n",
        "        if(user_choice == \"both\" or user_choice==\"lane\"):\n",
        "          lane_points = adjust_lane_points(frame_width, frame_height)\n",
        "          draw_lane_overlay(frame, lane_points)\n",
        "\n",
        "        out.write(frame)\n",
        "        pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Output video saved to {output_video_path}\")\n",
        "    pbar.close()\n",
        "    out.release()\n",
        "    # cv2.destroyAllWindows()\n",
        "\n",
        "def main():\n",
        "    emma_model = EMMA(cnn_feature_dim, intent_dim, historical_state_dim, hidden_size)  # Initialize the EMMA model\n",
        "    user_choice= input(\"Want to see lane or objects?\")\n",
        "    video_path = '/content/videos_train_00002.mp4'\n",
        "    output_video_path = '/content/emma_processedvideos.mp4'\n",
        "    if user_choice == \"box\":\n",
        "      output_video_path = '/content/emma_box_processedvideos.mp4'  # Path to the video file\n",
        "    process_video(video_path, output_video_path, emma_model, intent_dim, historical_state_dim,user_choice)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npr5Ui8b_8k2"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}